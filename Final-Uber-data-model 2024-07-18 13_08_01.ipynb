{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58cce1f0-b154-427b-8545-ee419fa2ce4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "341346f3-2c2f-4978-95c4-36fde84bb965",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create or get SparkSession (assuming you already have it created)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Replace with the actual name of your table\n",
    "table_name = \"default.uber_data_4_csv\"  # Update this with your table name\n",
    "\n",
    "# Load the DataFrame from the table\n",
    "df = spark.table(table_name)\n",
    "\n",
    "# Convert Spark DataFrame to pandas DataFrame (assuming Arrow is enabled)\n",
    "pandas_df = df.toPandas()\n",
    "\n",
    "# ... (rest of your code using pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1390cee-3bd5-4c92-9ac6-e399b5a905b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp\n",
    "df = df.withColumn(\"tpep_pickup_datetime\", to_timestamp(col(\"tpep_pickup_datetime\")))\n",
    "df = df.withColumn(\"tpep_dropoff_datetime\", to_timestamp(col(\"tpep_dropoff_datetime\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52664d3c-2920-4a32-9922-67f9be06711e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "df = df.withColumn(\"trip_id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d3db71d-9cb4-4bdf-80af-fd7fd65a2b6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+---------+--------+----------+---------+------------+---------------------+---------+--------+----------+---------+------------+\n|datetime_id|tpep_pickup_datetime|pick_hour|pick_day|pick_month|pick_year|pick_weekday|tpep_dropoff_datetime|drop_hour|drop_day|drop_month|drop_year|drop_weekday|\n+-----------+--------------------+---------+--------+----------+---------+------------+---------------------+---------+--------+----------+---------+------------+\n|          0| 2016-03-10 07:07:32|        7|      10|         3|     2016|           5|  2016-03-10 07:23:35|        7|      10|         3|     2016|           5|\n|          1| 2016-03-10 07:07:56|        7|      10|         3|     2016|           5|  2016-03-10 07:22:02|        7|      10|         3|     2016|           5|\n|          2| 2016-03-10 07:09:03|        7|      10|         3|     2016|           5|  2016-03-10 07:15:34|        7|      10|         3|     2016|           5|\n|          3| 2016-03-10 07:09:23|        7|      10|         3|     2016|           5|  2016-03-10 07:25:00|        7|      10|         3|     2016|           5|\n|          4| 2016-03-10 07:09:35|        7|      10|         3|     2016|           5|  2016-03-10 07:14:29|        7|      10|         3|     2016|           5|\n|          5| 2016-03-10 07:11:33|        7|      10|         3|     2016|           5|  2016-03-10 07:20:24|        7|      10|         3|     2016|           5|\n|          6| 2016-03-10 07:11:49|        7|      10|         3|     2016|           5|  2016-03-10 07:22:27|        7|      10|         3|     2016|           5|\n|          7| 2016-03-10 07:14:05|        7|      10|         3|     2016|           5|  2016-03-10 07:24:33|        7|      10|         3|     2016|           5|\n|          8| 2016-03-10 07:14:06|        7|      10|         3|     2016|           5|  2016-03-10 07:19:48|        7|      10|         3|     2016|           5|\n|          9| 2016-03-10 07:14:30|        7|      10|         3|     2016|           5|  2016-03-10 07:18:18|        7|      10|         3|     2016|           5|\n|         10| 2016-03-10 07:16:46|        7|      10|         3|     2016|           5|  2016-03-10 07:41:16|        7|      10|         3|     2016|           5|\n|         11| 2016-03-10 07:18:54|        7|      10|         3|     2016|           5|  2016-03-10 07:34:40|        7|      10|         3|     2016|           5|\n|         12| 2016-03-10 07:19:45|        7|      10|         3|     2016|           5|  2016-03-10 07:39:47|        7|      10|         3|     2016|           5|\n|         13| 2016-03-10 07:20:43|        7|      10|         3|     2016|           5|  2016-03-10 07:26:58|        7|      10|         3|     2016|           5|\n|         14| 2016-03-10 07:22:55|        7|      10|         3|     2016|           5|  2016-03-10 07:26:41|        7|      10|         3|     2016|           5|\n|         15| 2016-03-10 07:26:56|        7|      10|         3|     2016|           5|  2016-03-10 07:42:48|        7|      10|         3|     2016|           5|\n|         16| 2016-03-10 07:30:10|        7|      10|         3|     2016|           5|  2016-03-10 07:33:28|        7|      10|         3|     2016|           5|\n|         17| 2016-03-10 07:31:02|        7|      10|         3|     2016|           5|  2016-03-10 07:50:18|        7|      10|         3|     2016|           5|\n|         18| 2016-03-10 07:33:15|        7|      10|         3|     2016|           5|  2016-03-10 07:39:53|        7|      10|         3|     2016|           5|\n|         19| 2016-03-10 07:33:52|        7|      10|         3|     2016|           5|  2016-03-10 07:53:11|        7|      10|         3|     2016|           5|\n+-----------+--------------------+---------+--------+----------+---------+------------+---------------------+---------+--------+----------+---------+------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, year, month, dayofmonth, hour, dayofweek, monotonically_increasing_id\n",
    "\n",
    "# Create datetime_dim DataFrame\n",
    "datetime_dim = df.select(col(\"tpep_pickup_datetime\"), col(\"tpep_dropoff_datetime\"))\n",
    "\n",
    "# Drop duplicates\n",
    "datetime_dim = datetime_dim.dropDuplicates()\n",
    "\n",
    "# Extract datetime components for pickup\n",
    "datetime_dim = datetime_dim.withColumn(\"pick_hour\", hour(col(\"tpep_pickup_datetime\")))\n",
    "datetime_dim = datetime_dim.withColumn(\"pick_day\", dayofmonth(col(\"tpep_pickup_datetime\")))\n",
    "datetime_dim = datetime_dim.withColumn(\"pick_month\", month(col(\"tpep_pickup_datetime\")))\n",
    "datetime_dim = datetime_dim.withColumn(\"pick_year\", year(col(\"tpep_pickup_datetime\")))\n",
    "datetime_dim = datetime_dim.withColumn(\"pick_weekday\", dayofweek(col(\"tpep_pickup_datetime\")))\n",
    "\n",
    "\n",
    "# Extract datetime components for dropoff (similar operations)\n",
    "datetime_dim = datetime_dim.withColumn(\"drop_hour\", hour(col(\"tpep_dropoff_datetime\")))\n",
    "datetime_dim = datetime_dim.withColumn(\"drop_day\", dayofmonth(col(\"tpep_dropoff_datetime\")))\n",
    "datetime_dim = datetime_dim.withColumn(\"drop_month\", month(col(\"tpep_dropoff_datetime\")))\n",
    "datetime_dim = datetime_dim.withColumn(\"drop_year\", year(col(\"tpep_dropoff_datetime\")))\n",
    "datetime_dim = datetime_dim.withColumn(\"drop_weekday\", dayofweek(col(\"tpep_dropoff_datetime\")))\n",
    "\n",
    "# Create datetime_id using index\n",
    "datetime_dim = datetime_dim.withColumn(\"datetime_id\", monotonically_increasing_id())\n",
    "\n",
    "# Reorder columns (optional)\n",
    "datetime_dim = datetime_dim.select(\"datetime_id\", \"tpep_pickup_datetime\", \"pick_hour\", \"pick_day\", \"pick_month\", \"pick_year\", \"pick_weekday\", \"tpep_dropoff_datetime\", \"drop_hour\", \"drop_day\", \"drop_month\", \"drop_year\", \"drop_weekday\")\n",
    "\n",
    "# Display the first few rows\n",
    "datetime_dim.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed2c7119-b7b7-4ce2-8cce-6a63d1746a74",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "# Create passenger_count_dim\n",
    "passenger_count_dim = df.select(\"passenger_count\")  # Select only the passenger_count column\n",
    "passenger_count_dim = passenger_count_dim.withColumn(\"passenger_count_id\", monotonically_increasing_id())  # Add ID using index\n",
    "passenger_count_dim = passenger_count_dim.select(\"passenger_count_id\", \"passenger_count\")  # Reorder columns (optional)\n",
    "\n",
    "# Create trip_distance_dim (similar approach)\n",
    "trip_distance_dim = df.select(\"trip_distance\")\n",
    "trip_distance_dim = trip_distance_dim.withColumn(\"trip_distance_id\", monotonically_increasing_id())\n",
    "trip_distance_dim = trip_distance_dim.select(\"trip_distance_id\", \"trip_distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c65caa7-fff5-4281-88b7-66f124485efb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+\n|passenger_count_id|passenger_count|\n+------------------+---------------+\n|                 0|              1|\n|                 1|              1|\n|                 2|              1|\n|                 3|              1|\n|                 4|              1|\n|                 5|              1|\n|                 6|              2|\n|                 7|              3|\n|                 8|              5|\n|                 9|              2|\n|                10|              1|\n|                11|              6|\n|                12|              6|\n|                13|              1|\n|                14|              1|\n|                15|              4|\n|                16|              6|\n|                17|              2|\n|                18|              1|\n|                19|              1|\n+------------------+---------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "passenger_count_dim.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1425fb50-e83f-4a5a-b81c-c00a00f2561c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+--------------+\n|rate_code_id|RatecodeID|rate_code_name|\n+------------+----------+--------------+\n|           0|         1| Standard rate|\n|           1|         1| Standard rate|\n|           2|         1| Standard rate|\n|           3|         1| Standard rate|\n|           4|         1| Standard rate|\n|           5|         1| Standard rate|\n|           6|         1| Standard rate|\n|           7|         1| Standard rate|\n|           8|         1| Standard rate|\n|           9|         1| Standard rate|\n|          10|         1| Standard rate|\n|          11|         1| Standard rate|\n|          12|         1| Standard rate|\n|          13|         1| Standard rate|\n|          14|         1| Standard rate|\n|          15|         1| Standard rate|\n|          16|         1| Standard rate|\n|          17|         1| Standard rate|\n|          18|         1| Standard rate|\n|          19|         1| Standard rate|\n+------------+----------+--------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lit, when\n",
    "\n",
    "# Define the rate code mapping as a dictionary\n",
    "rate_code_map = {\n",
    "  1: \"Standard rate\",\n",
    "  2: \"JFK\",\n",
    "  3: \"Newark\",\n",
    "  4: \"Nassau or Westchester\",\n",
    "  5: \"Negotiated fare\",\n",
    "  6: \"Group ride\"\n",
    "}\n",
    "\n",
    "# Broadcast the rate code mapping dictionary for efficiency\n",
    "rate_code_map_broadcast = spark.sparkContext.broadcast(rate_code_map)\n",
    "\n",
    "# Create rate_code_dim\n",
    "rate_code_dim = df.select(\"RatecodeID\")  # Select only the RatecodeID column\n",
    "rate_code_dim = rate_code_dim.withColumn(\"rate_code_id\", monotonically_increasing_id())  # Add ID using index\n",
    "rate_code_dim = rate_code_dim.withColumn(\"rate_code_name\", \n",
    "                                         when(col(\"RatecodeID\").isin(*rate_code_map.keys()), \n",
    "                                              lit(None)).otherwise(lit(\"Unknown\")))\n",
    "\n",
    "# Map rate code to name using UDF (User-Defined Function)\n",
    "def map_rate_code(rate_code):\n",
    "    return rate_code_map_broadcast.value.get(rate_code, \"Unknown\")\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "map_rate_code_udf = udf(map_rate_code, StringType())\n",
    "\n",
    "rate_code_dim = rate_code_dim.withColumn(\"rate_code_name\", map_rate_code_udf(col(\"RatecodeID\")))\n",
    "\n",
    "# Reorder columns (optional)\n",
    "rate_code_dim = rate_code_dim.select(\"rate_code_id\", \"RatecodeID\", \"rate_code_name\")\n",
    "\n",
    "# Display rate_code_dim\n",
    "rate_code_dim.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5fabce4-7f9a-4ee6-87e7-7ab1321c8211",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+-----------------+\n|payment_type_id|payment_type|payment_type_name|\n+---------------+------------+-----------------+\n|              0|           1|          Unknown|\n|              1|           1|          Unknown|\n|              2|           1|          Unknown|\n|              3|           2|          Unknown|\n|              4|           1|          Unknown|\n|              5|           2|          Unknown|\n|              6|           2|          Unknown|\n|              7|           1|          Unknown|\n|              8|           1|          Unknown|\n|              9|           1|          Unknown|\n|             10|           2|          Unknown|\n|             11|           1|          Unknown|\n|             12|           2|          Unknown|\n|             13|           1|          Unknown|\n|             14|           1|          Unknown|\n|             15|           1|          Unknown|\n|             16|           1|          Unknown|\n|             17|           1|          Unknown|\n|             18|           2|          Unknown|\n|             19|           1|          Unknown|\n+---------------+------------+-----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, monotonically_increasing_id, lit, when\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "payment_type_name = {\n",
    "    1: \"Credit card\",\n",
    "    2: \"Cash\",\n",
    "    3: \"No charge\",\n",
    "    4: \"Dispute\",\n",
    "    5: \"Unknown\",\n",
    "    6: \"Voided trip\"\n",
    "}\n",
    "\n",
    "# Broadcast the payment type mapping dictionary for efficiency\n",
    "payment_type_name_broadcast = spark.sparkContext.broadcast(payment_type_name)\n",
    "\n",
    "# Create payment_type_dim\n",
    "payment_type_dim = df.select(\"payment_type\")  # Select only the payment_type column\n",
    "payment_type_dim = payment_type_dim.withColumn(\"payment_type_id\", monotonically_increasing_id())  # Add ID using index\n",
    "\n",
    "# Map payment type to name using UDF (User-Defined Function)\n",
    "def map_payment_type(payment_type):\n",
    "    return payment_type_name_broadcast.value.get(payment_type, \"Unknown\")\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "map_payment_type_udf = udf(map_payment_type, StringType())\n",
    "\n",
    "payment_type_dim = payment_type_dim.withColumn(\"payment_type_name\", map_payment_type_udf(col(\"payment_type\")))\n",
    "\n",
    "# Reorder columns (optional)\n",
    "payment_type_dim = payment_type_dim.select(\"payment_type_id\", \"payment_type\", \"payment_type_name\")\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "payment_type_dim.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa4ae431-5453-4dc1-9879-eb5bd18731ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+----------------+\n|pickup_location_id|pickup_latitude|pickup_longitude|\n+------------------+---------------+----------------+\n|                 0|       40.60926|      -74.651306|\n|                 1|       40.74684|       -73.98588|\n|                 2|      40.717075|       -73.99157|\n|                 3|       40.75896|        -73.9843|\n|                 4|       40.73461|       -73.99866|\n|                 5|      40.755627|        -73.9906|\n|                 6|       40.75195|      -73.975075|\n|                 7|      40.771076|       -73.86641|\n|                 8|       40.76581|       -73.92645|\n|                 9|      40.760414|       -74.00295|\n|                10|       40.76104|       -73.98288|\n|                11|      40.767204|       -73.96251|\n|                12|      40.780525|      -73.949036|\n|                13|       40.77373|        -73.8708|\n|                14|       40.76596|       -73.96334|\n|                15|       40.76556|       -73.96624|\n|                16|       40.77668|       -73.94955|\n|                17|       40.76249|       -73.92073|\n|                18|       40.75993|       -73.96497|\n|                19|      40.753445|      -73.977104|\n+------------------+---------------+----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, monotonically_increasing_id\n",
    "\n",
    "# Create pickup_location_dim\n",
    "pickup_location_dim = df.select(\"pickup_longitude\", \"pickup_latitude\")  # Select only the desired columns\n",
    "pickup_location_dim = pickup_location_dim.withColumn(\"pickup_location_id\", monotonically_increasing_id())  # Add ID using index\n",
    "pickup_location_dim = pickup_location_dim.select(\"pickup_location_id\", \"pickup_latitude\", \"pickup_longitude\")  # Reorder columns (optional)\n",
    "\n",
    "# Display pickup_location_dim\n",
    "pickup_location_dim.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6851248-143e-497c-9123-cd322b890331",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------+-----------------+\n|dropoff_location_id|dropoff_latitude|dropoff_longitude|\n+-------------------+----------------+-----------------+\n|                  0|        40.60926|       -74.651306|\n|                  1|        40.70756|        -74.00725|\n|                  2|        40.72248|        -73.98078|\n|                  3|        40.76184|        -73.99066|\n|                  4|        40.76428|        -73.97527|\n|                  5|       40.763397|        -73.97066|\n|                  6|        40.74193|        -73.98673|\n|                  7|        40.63352|        -74.01157|\n|                  8|        40.73465|        -73.98324|\n|                  9|        40.75079|        -73.98292|\n|                 10|        40.75462|        -73.97627|\n|                 11|       40.789665|        -73.96607|\n|                 12|        40.79473|       -73.943756|\n|                 13|        40.75993|        -73.97884|\n|                 14|        40.77372|        -73.96183|\n|                 15|        40.74166|        -73.97497|\n|                 16|         40.7656|        -73.95475|\n|                 17|        40.74196|        -73.97479|\n|                 18|        40.75993|        -73.96497|\n|                 19|       40.747723|        -73.98884|\n+-------------------+----------------+-----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, monotonically_increasing_id\n",
    "\n",
    "# Create dropoff_location_dim\n",
    "dropoff_location_dim = df.select(\"dropoff_longitude\", \"dropoff_latitude\")  # Select only the desired columns\n",
    "dropoff_location_dim = dropoff_location_dim.withColumn(\"dropoff_location_id\", monotonically_increasing_id())  # Add ID using index\n",
    "dropoff_location_dim = dropoff_location_dim.select(\"dropoff_location_id\", \"dropoff_latitude\", \"dropoff_longitude\")  # Reorder columns (optional)\n",
    "\n",
    "# Display dropoff_location_dim\n",
    "dropoff_location_dim.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef917b72-c1c3-4044-8c8d-3d21e16e401a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----------+------------------+----------------+------------+------------------+------------------+-------------------+---------------+-----------+-----+-------+----------+------------+---------------------+------------+\n|trip_id|VendorID|datetime_id|passenger_count_id|trip_distance_id|rate_code_id|store_and_fwd_flag|pickup_location_id|dropoff_location_id|payment_type_id|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|\n+-------+--------+-----------+------------------+----------------+------------+------------------+------------------+-------------------+---------------+-----------+-----+-------+----------+------------+---------------------+------------+\n|      0|       2|          0|                 0|               0|           0|                 N|                 0|                  0|              0|        6.5|  0.5|    0.5|       1.0|         0.0|                  0.3|         8.8|\n|      1|       2|          1|                 1|               1|           1|                 N|                 1|                  1|              1|       12.0|  0.5|    0.5|       2.0|         0.0|                  0.3|        15.3|\n|      2|       1|          2|                 2|               2|           2|                 N|                 2|                  2|              2|        5.0|  0.5|    0.5|      1.25|         0.0|                  0.3|        7.55|\n|      3|       2|          3|                 3|               3|           3|                 N|                 3|                  3|              3|        5.0|  0.0|    0.5|       0.0|         0.0|                  0.3|         5.8|\n|      4|       2|          4|                 4|               4|           4|                 N|                 4|                  4|              4|       10.5|  0.0|    0.5|      2.26|         0.0|                  0.3|       13.56|\n|      5|       2|          5|                 5|               5|           5|                 N|                 5|                  5|              5|        8.0|  0.0|    0.5|       0.0|         0.0|                  0.3|         8.8|\n|      6|       2|          6|                 6|               6|           6|                 N|                 6|                  6|              6|        5.0|  0.0|    0.5|       0.0|         0.0|                  0.3|         5.8|\n|      7|       2|          7|                 7|               7|           7|                 N|                 7|                  7|              7|       47.5|  0.0|    0.5|      9.66|         0.0|                  0.3|       57.96|\n|      8|       2|          8|                 8|               8|           8|                 N|                 8|                  8|              8|       18.5|  0.0|    0.5|      3.86|         0.0|                  0.3|       23.16|\n|      9|       2|          9|                 9|               9|           9|                 N|                 9|                  9|              9|        8.0|  0.0|    0.5|      2.64|         0.0|                  0.3|       11.44|\n|     10|       2|         10|                10|              10|          10|                 N|                10|                 10|             10|        5.0|  0.0|    0.5|       0.0|         0.0|                  0.3|         5.8|\n|     11|       2|         11|                11|              11|          11|                 N|                11|                 11|             11|       11.5|  0.0|    0.5|       1.0|         0.0|                  0.3|        13.3|\n|     12|       2|         12|                12|              12|          12|                 N|                12|                 12|             12|        6.0|  0.0|    0.5|       0.0|         0.0|                  0.3|         6.8|\n|     13|       2|         13|                13|              13|          13|                 N|                13|                 13|             13|       38.0|  0.0|    0.5|       4.0|        5.54|                  0.3|       48.34|\n|     14|       2|         14|                14|              14|          14|                 N|                14|                 14|             14|        5.0|  0.0|    0.5|      1.16|         0.0|                  0.3|        6.96|\n|     15|       2|         15|                15|              15|          15|                 N|                15|                 15|             15|        9.0|  0.0|    0.5|       1.0|         0.0|                  0.3|        10.8|\n|     16|       2|         16|                16|              16|          16|                 N|                16|                 16|             16|        6.5|  0.0|    0.5|      1.82|         0.0|                  0.3|        9.12|\n|     17|       2|         17|                17|              17|          17|                 N|                17|                 17|             17|       20.0|  0.0|    0.5|      5.27|        5.54|                  0.3|       31.61|\n|     18|       2|         18|                18|              18|          18|                 N|                18|                 18|             18|        2.5|  0.0|    0.5|       0.0|         0.0|                  0.3|         3.3|\n|     19|       2|         19|                19|              19|          19|                 N|                19|                 19|             19|        5.5|  0.0|    0.5|      1.26|         0.0|                  0.3|        7.56|\n+-------+--------+-----------+------------------+----------------+------------+------------------+------------------+-------------------+---------------+-----------+-----+-------+----------+------------+---------------------+------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "passenger_count_dim = df.select(\"trip_id\").withColumnRenamed(\"trip_id\", \"passenger_count_id\")\n",
    "trip_distance_dim = df.select(\"trip_id\").withColumnRenamed(\"trip_id\", \"trip_distance_id\")\n",
    "rate_code_dim = df.select(\"trip_id\").withColumnRenamed(\"trip_id\", \"rate_code_id\")\n",
    "pickup_location_dim = df.select(\"trip_id\").withColumnRenamed(\"trip_id\", \"pickup_location_id\")\n",
    "dropoff_location_dim = df.select(\"trip_id\").withColumnRenamed(\"trip_id\", \"dropoff_location_id\")\n",
    "datetime_dim = df.select(\"trip_id\").withColumnRenamed(\"trip_id\", \"datetime_id\")\n",
    "payment_type_dim = df.select(\"trip_id\").withColumnRenamed(\"trip_id\", \"payment_type_id\")\n",
    "\n",
    "# Perform the joins to create the fact table\n",
    "fact_table = df \\\n",
    "    .join(passenger_count_dim, df.trip_id == passenger_count_dim.passenger_count_id) \\\n",
    "    .join(trip_distance_dim, df.trip_id == trip_distance_dim.trip_distance_id) \\\n",
    "    .join(rate_code_dim, df.trip_id == rate_code_dim.rate_code_id) \\\n",
    "    .join(pickup_location_dim, df.trip_id == pickup_location_dim.pickup_location_id) \\\n",
    "    .join(dropoff_location_dim, df.trip_id == dropoff_location_dim.dropoff_location_id) \\\n",
    "    .join(datetime_dim, df.trip_id == datetime_dim.datetime_id) \\\n",
    "    .join(payment_type_dim, df.trip_id == payment_type_dim.payment_type_id) \\\n",
    "    .select(\n",
    "        df.trip_id, df.VendorID, datetime_dim.datetime_id, passenger_count_dim.passenger_count_id,\n",
    "        trip_distance_dim.trip_distance_id, rate_code_dim.rate_code_id, df.store_and_fwd_flag,\n",
    "        pickup_location_dim.pickup_location_id, dropoff_location_dim.dropoff_location_id,\n",
    "        payment_type_dim.payment_type_id, df.fare_amount, df.extra, df.mta_tax, df.tip_amount,\n",
    "        df.tolls_amount, df.improvement_surcharge, df.total_amount\n",
    "    )\n",
    "\n",
    "# Display the resulting fact table\n",
    "fact_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c03a720-aabd-4439-b210-158c09d51ead",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- Create the fact_table\n",
    "CREATE TABLE fact_table (\n",
    "  trip_id INT,\n",
    "  VendorID INT,\n",
    "  datetime_id INT,\n",
    "  passenger_count_id INT,\n",
    "  trip_distance_id INT,\n",
    "  rate_code_id INT,\n",
    "  store_and_fwd_flag STRING,\n",
    "  pickup_location_id INT,\n",
    "  dropoff_location_id INT,\n",
    "  payment_type_id INT,\n",
    "  fare_amount DOUBLE,\n",
    "  extra DOUBLE,\n",
    "  mta_tax DOUBLE,\n",
    "  tip_amount DOUBLE,\n",
    "  tolls_amount DOUBLE,\n",
    "  improvement_surcharge DOUBLE,\n",
    "  total_amount DOUBLE\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bdb95c2-3dbc-4176-b909-77d54f9cdd40",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>dropoff_location_id</th><th>dropoff_latitude</th><th>dropoff_longitude</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "dropoff_location_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "dropoff_latitude",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "dropoff_longitude",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from default.dropoff_location_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7a34faa-dcdd-43fc-9e18-066b60af572a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-546318218290649>:21\u001B[0m\n",
       "\u001B[1;32m     19\u001B[0m     display(df)\n",
       "\u001B[1;32m     20\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
       "\u001B[0;32m---> 21\u001B[0m   _sqldf \u001B[38;5;241m=\u001B[39m \u001B[43m____databricks_percent_sql\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m     23\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m ____databricks_percent_sql\n",
       "\n",
       "File \u001B[0;32m<command-546318218290649>:4\u001B[0m, in \u001B[0;36m____databricks_percent_sql\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m____databricks_percent_sql\u001B[39m():\n",
       "\u001B[1;32m      3\u001B[0m   \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mbase64\u001B[39;00m\n",
       "\u001B[0;32m----> 4\u001B[0m   \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase64\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstandard_b64decode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mLS0gQXNzdW1pbmcgeW91ciB0YWJsZXMgKGRpbWVuc2lvbiBhbmQgZmFjdCkgYWxyZWFkeSBleGlzdAoKLS0gMS4gQ3JlYXRlIHRlbXBvcmFyeSB2aWV3cyBmb3IgZGltZW5zaW9uIHRhYmxlczoKQ1JFQVRFIE9SIFJFUExBQ0UgVEVNUCBWSUVXIHBhc3Nlbmdlcl9jb3VudF9kaW0gQVMKU0VMRUNUIHRyaXBfaWQgQVMgcGFzc2VuZ2VyX2NvdW50X2lkLCBwYXNzZW5nZXJfY291bnQKRlJPTSBwYXNzZW5nZXJfY291bnRfZGlt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m      5\u001B[0m   spark\u001B[38;5;241m.\u001B[39msql(base64\u001B[38;5;241m.\u001B[39mstandard_b64decode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mQ1JFQVRFIE9SIFJFUExBQ0UgVEVNUCBWSUVXIHRyaXBfZGlzdGFuY2VfZGltIEFTClNFTEVDVCB0cmlwX2lkIEFTIHRyaXBfZGlzdGFuY2VfaWQsIHRyaXBfZGlzdGFuY2UKRlJPTSB0cmlwX2Rpc3RhbmNlX2RpbQ==\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mdecode())\n",
       "\u001B[1;32m      6\u001B[0m   spark\u001B[38;5;241m.\u001B[39msql(base64\u001B[38;5;241m.\u001B[39mstandard_b64decode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mQ1JFQVRFIE9SIFJFUExBQ0UgVEVNUCBWSUVXIHJhdGVfY29kZV9kaW0gQVMKU0VMRUNUIHRyaXBfaWQgQVMgcmF0ZV9jb2RlX2lkLCBSYXRlY29kZUlELCByYXRlX2NvZGVfbmFtZQpGUk9NIHJhdGVfY29kZV9kaW0=\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mdecode())\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n",
       "\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n",
       "\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `trip_id` cannot be resolved. Did you mean one of the following? [`passenger_count_dim`.`passenger_count_id`].; line 5 pos 7;\n",
       "'CreateViewCommand `passenger_count_dim`, SELECT trip_id AS passenger_count_id, passenger_count\n",
       "FROM passenger_count_dim, false, true, LocalTempView, false, false, false\n",
       "+- 'Project ['trip_id AS passenger_count_id#8423, 'passenger_count]\n",
       "   +- SubqueryAlias passenger_count_dim\n",
       "      +- View (`passenger_count_dim`, [passenger_count_id#1835L])\n",
       "         +- Project [trip_id#1331L AS passenger_count_id#1835L]\n",
       "            +- Project [trip_id#1331L]\n",
       "               +- Project [VendorID#1195, tpep_pickup_datetime#1291, tpep_dropoff_datetime#1311, passenger_count#1198, trip_distance#1199, pickup_longitude#1200, pickup_latitude#1201, RatecodeID#1202, store_and_fwd_flag#1203, dropoff_longitude#1204, dropoff_latitude#1205, payment_type#1206, fare_amount#1207, extra#1208, mta_tax#1209, tip_amount#1210, tolls_amount#1211, improvement_surcharge#1212, total_amount#1213, monotonically_increasing_id() AS trip_id#1331L]\n",
       "                  +- Deduplicate [improvement_surcharge#1212, tpep_dropoff_datetime#1311, trip_distance#1199, dropoff_longitude#1204, pickup_latitude#1201, tolls_amount#1211, RatecodeID#1202, VendorID#1195, tip_amount#1210, payment_type#1206, fare_amount#1207, pickup_longitude#1200, passenger_count#1198, store_and_fwd_flag#1203, extra#1208, total_amount#1213, tpep_pickup_datetime#1291, dropoff_latitude#1205, mta_tax#1209]\n",
       "                     +- Project [VendorID#1195, tpep_pickup_datetime#1291, to_timestamp(tpep_dropoff_datetime#1197, None, TimestampType, Some(Etc/UTC), false) AS tpep_dropoff_datetime#1311, passenger_count#1198, trip_distance#1199, pickup_longitude#1200, pickup_latitude#1201, RatecodeID#1202, store_and_fwd_flag#1203, dropoff_longitude#1204, dropoff_latitude#1205, payment_type#1206, fare_amount#1207, extra#1208, mta_tax#1209, tip_amount#1210, tolls_amount#1211, improvement_surcharge#1212, total_amount#1213]\n",
       "                        +- Project [VendorID#1195, to_timestamp(tpep_pickup_datetime#1196, None, TimestampType, Some(Etc/UTC), false) AS tpep_pickup_datetime#1291, tpep_dropoff_datetime#1197, passenger_count#1198, trip_distance#1199, pickup_longitude#1200, pickup_latitude#1201, RatecodeID#1202, store_and_fwd_flag#1203, dropoff_longitude#1204, dropoff_latitude#1205, payment_type#1206, fare_amount#1207, extra#1208, mta_tax#1209, tip_amount#1210, tolls_amount#1211, improvement_surcharge#1212, total_amount#1213]\n",
       "                           +- SubqueryAlias spark_catalog.default.uber_data_4_csv\n",
       "                              +- Relation spark_catalog.default.uber_data_4_csv[VendorID#1195,tpep_pickup_datetime#1196,tpep_dropoff_datetime#1197,passenger_count#1198,trip_distance#1199,pickup_longitude#1200,pickup_latitude#1201,RatecodeID#1202,store_and_fwd_flag#1203,dropoff_longitude#1204,dropoff_latitude#1205,payment_type#1206,fare_amount#1207,extra#1208,mta_tax#1209,tip_amount#1210,tolls_amount#1211,improvement_surcharge#1212,total_amount#1213] csv\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-546318218290649>:21\u001B[0m\n\u001B[1;32m     19\u001B[0m     display(df)\n\u001B[1;32m     20\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m df\n\u001B[0;32m---> 21\u001B[0m   _sqldf \u001B[38;5;241m=\u001B[39m \u001B[43m____databricks_percent_sql\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     23\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m ____databricks_percent_sql\n\nFile \u001B[0;32m<command-546318218290649>:4\u001B[0m, in \u001B[0;36m____databricks_percent_sql\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m____databricks_percent_sql\u001B[39m():\n\u001B[1;32m      3\u001B[0m   \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mbase64\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m   \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase64\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstandard_b64decode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mLS0gQXNzdW1pbmcgeW91ciB0YWJsZXMgKGRpbWVuc2lvbiBhbmQgZmFjdCkgYWxyZWFkeSBleGlzdAoKLS0gMS4gQ3JlYXRlIHRlbXBvcmFyeSB2aWV3cyBmb3IgZGltZW5zaW9uIHRhYmxlczoKQ1JFQVRFIE9SIFJFUExBQ0UgVEVNUCBWSUVXIHBhc3Nlbmdlcl9jb3VudF9kaW0gQVMKU0VMRUNUIHRyaXBfaWQgQVMgcGFzc2VuZ2VyX2NvdW50X2lkLCBwYXNzZW5nZXJfY291bnQKRlJPTSBwYXNzZW5nZXJfY291bnRfZGlt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m   spark\u001B[38;5;241m.\u001B[39msql(base64\u001B[38;5;241m.\u001B[39mstandard_b64decode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mQ1JFQVRFIE9SIFJFUExBQ0UgVEVNUCBWSUVXIHRyaXBfZGlzdGFuY2VfZGltIEFTClNFTEVDVCB0cmlwX2lkIEFTIHRyaXBfZGlzdGFuY2VfaWQsIHRyaXBfZGlzdGFuY2UKRlJPTSB0cmlwX2Rpc3RhbmNlX2RpbQ==\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mdecode())\n\u001B[1;32m      6\u001B[0m   spark\u001B[38;5;241m.\u001B[39msql(base64\u001B[38;5;241m.\u001B[39mstandard_b64decode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mQ1JFQVRFIE9SIFJFUExBQ0UgVEVNUCBWSUVXIHJhdGVfY29kZV9kaW0gQVMKU0VMRUNUIHRyaXBfaWQgQVMgcmF0ZV9jb2RlX2lkLCBSYXRlY29kZUlELCByYXRlX2NvZGVfbmFtZQpGUk9NIHJhdGVfY29kZV9kaW0=\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mdecode())\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `trip_id` cannot be resolved. Did you mean one of the following? [`passenger_count_dim`.`passenger_count_id`].; line 5 pos 7;\n'CreateViewCommand `passenger_count_dim`, SELECT trip_id AS passenger_count_id, passenger_count\nFROM passenger_count_dim, false, true, LocalTempView, false, false, false\n+- 'Project ['trip_id AS passenger_count_id#8423, 'passenger_count]\n   +- SubqueryAlias passenger_count_dim\n      +- View (`passenger_count_dim`, [passenger_count_id#1835L])\n         +- Project [trip_id#1331L AS passenger_count_id#1835L]\n            +- Project [trip_id#1331L]\n               +- Project [VendorID#1195, tpep_pickup_datetime#1291, tpep_dropoff_datetime#1311, passenger_count#1198, trip_distance#1199, pickup_longitude#1200, pickup_latitude#1201, RatecodeID#1202, store_and_fwd_flag#1203, dropoff_longitude#1204, dropoff_latitude#1205, payment_type#1206, fare_amount#1207, extra#1208, mta_tax#1209, tip_amount#1210, tolls_amount#1211, improvement_surcharge#1212, total_amount#1213, monotonically_increasing_id() AS trip_id#1331L]\n                  +- Deduplicate [improvement_surcharge#1212, tpep_dropoff_datetime#1311, trip_distance#1199, dropoff_longitude#1204, pickup_latitude#1201, tolls_amount#1211, RatecodeID#1202, VendorID#1195, tip_amount#1210, payment_type#1206, fare_amount#1207, pickup_longitude#1200, passenger_count#1198, store_and_fwd_flag#1203, extra#1208, total_amount#1213, tpep_pickup_datetime#1291, dropoff_latitude#1205, mta_tax#1209]\n                     +- Project [VendorID#1195, tpep_pickup_datetime#1291, to_timestamp(tpep_dropoff_datetime#1197, None, TimestampType, Some(Etc/UTC), false) AS tpep_dropoff_datetime#1311, passenger_count#1198, trip_distance#1199, pickup_longitude#1200, pickup_latitude#1201, RatecodeID#1202, store_and_fwd_flag#1203, dropoff_longitude#1204, dropoff_latitude#1205, payment_type#1206, fare_amount#1207, extra#1208, mta_tax#1209, tip_amount#1210, tolls_amount#1211, improvement_surcharge#1212, total_amount#1213]\n                        +- Project [VendorID#1195, to_timestamp(tpep_pickup_datetime#1196, None, TimestampType, Some(Etc/UTC), false) AS tpep_pickup_datetime#1291, tpep_dropoff_datetime#1197, passenger_count#1198, trip_distance#1199, pickup_longitude#1200, pickup_latitude#1201, RatecodeID#1202, store_and_fwd_flag#1203, dropoff_longitude#1204, dropoff_latitude#1205, payment_type#1206, fare_amount#1207, extra#1208, mta_tax#1209, tip_amount#1210, tolls_amount#1211, improvement_surcharge#1212, total_amount#1213]\n                           +- SubqueryAlias spark_catalog.default.uber_data_4_csv\n                              +- Relation spark_catalog.default.uber_data_4_csv[VendorID#1195,tpep_pickup_datetime#1196,tpep_dropoff_datetime#1197,passenger_count#1198,trip_distance#1199,pickup_longitude#1200,pickup_latitude#1201,RatecodeID#1202,store_and_fwd_flag#1203,dropoff_longitude#1204,dropoff_latitude#1205,payment_type#1206,fare_amount#1207,extra#1208,mta_tax#1209,tip_amount#1210,tolls_amount#1211,improvement_surcharge#1212,total_amount#1213] csv\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `trip_id` cannot be resolved. Did you mean one of the following? [`passenger_count_dim`.`passenger_count_id`].; line 5 pos 7;\n'CreateViewCommand `passenger_count_dim`, SELECT trip_id AS passenger_count_id, passenger_count\nFROM passenger_count_dim, false, true, LocalTempView, false, false, false\n+- 'Project ['trip_id AS passenger_count_id#8423, 'passenger_count]\n   +- SubqueryAlias passenger_count_dim\n      +- View (`passenger_count_dim`, [passenger_count_id#1835L])\n         +- Project [trip_id#1331L AS passenger_count_id#1835L]\n            +- Project [trip_id#1331L]\n               +- Project [VendorID#1195, tpep_pickup_datetime#1291, tpep_dropoff_datetime#1311, passenger_count#1198, trip_distance#1199, pickup_longitude#1200, pickup_latitude#1201, RatecodeID#1202, store_and_fwd_flag#1203, dropoff_longitude#1204, dropoff_latitude#1205, payment_type#1206, fare_amount#1207, extra#1208, mta_tax#1209, tip_amount#1210, tolls_amount#1211, improvement_surcharge#1212, total_amount#1213, monotonically_increasing_id() AS trip_id#1331L]\n                  +- Deduplicate [improvement_surcharge#1212, tpep_dropoff_datetime#1311, trip_distance#1199, dropoff_longitude#1204, pickup_latitude#1201, tolls_amount#1211, RatecodeID#1202, VendorID#1195, tip_amount#1210, payment_type#1206, fare_amount#1207, pickup_longitude#1200, passenger_count#1198, store_and_fwd_flag#1203, extra#1208, total_amount#1213, tpep_pickup_datetime#1291, dropoff_latitude#1205, mta_tax#1209]\n                     +- Project [VendorID#1195, tpep_pickup_datetime#1291, to_timestamp(tpep_dropoff_datetime#1197, None, TimestampType, Some(Etc/UTC), false) AS tpep_dropoff_datetime#1311, passenger_count#1198, trip_distance#1199, pickup_longitude#1200, pickup_latitude#1201, RatecodeID#1202, store_and_fwd_flag#1203, dropoff_longitude#1204, dropoff_latitude#1205, payment_type#1206, fare_amount#1207, extra#1208, mta_tax#1209, tip_amount#1210, tolls_amount#1211, improvement_surcharge#1212, total_amount#1213]\n                        +- Project [VendorID#1195, to_timestamp(tpep_pickup_datetime#1196, None, TimestampType, Some(Etc/UTC), false) AS tpep_pickup_datetime#1291, tpep_dropoff_datetime#1197, passenger_count#1198, trip_distance#1199, pickup_longitude#1200, pickup_latitude#1201, RatecodeID#1202, store_and_fwd_flag#1203, dropoff_longitude#1204, dropoff_latitude#1205, payment_type#1206, fare_amount#1207, extra#1208, mta_tax#1209, tip_amount#1210, tolls_amount#1211, improvement_surcharge#1212, total_amount#1213]\n                           +- SubqueryAlias spark_catalog.default.uber_data_4_csv\n                              +- Relation spark_catalog.default.uber_data_4_csv[VendorID#1195,tpep_pickup_datetime#1196,tpep_dropoff_datetime#1197,passenger_count#1198,trip_distance#1199,pickup_longitude#1200,pickup_latitude#1201,RatecodeID#1202,store_and_fwd_flag#1203,dropoff_longitude#1204,dropoff_latitude#1205,payment_type#1206,fare_amount#1207,extra#1208,mta_tax#1209,tip_amount#1210,tolls_amount#1211,improvement_surcharge#1212,total_amount#1213] csv\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- Assuming your tables (dimension and fact) already exist\n",
    "\n",
    "-- 1. Create temporary views for dimension tables:\n",
    "CREATE OR REPLACE TEMP VIEW passenger_count_dim AS\n",
    "SELECT trip_id AS passenger_count_id, passenger_count\n",
    "FROM passenger_count_dim;\n",
    "\n",
    "CREATE OR REPLACE TEMP VIEW trip_distance_dim AS\n",
    "SELECT trip_id AS trip_distance_id, trip_distance\n",
    "FROM trip_distance_dim;\n",
    "\n",
    "CREATE OR REPLACE TEMP VIEW rate_code_dim AS\n",
    "SELECT trip_id AS rate_code_id, RatecodeID, rate_code_name\n",
    "FROM rate_code_dim;\n",
    "\n",
    "CREATE OR REPLACE TEMP VIEW payment_type_dim AS\n",
    "SELECT trip_id AS payment_type_id, payment_type, payment_type_name\n",
    "FROM payment_type_dim;\n",
    "\n",
    "CREATE OR REPLACE TEMP VIEW pickup_location_dim AS\n",
    "SELECT trip_id AS pickup_location_id, pickup_latitude, pickup_longitude\n",
    "FROM pickup_location_dim;\n",
    "\n",
    "CREATE OR REPLACE TEMP VIEW dropoff_location_dim AS\n",
    "SELECT trip_id AS dropoff_location_id, dropoff_latitude, dropoff_longitude\n",
    "FROM dropoff_location_dim;\n",
    "\n",
    "CREATE OR REPLACE TEMP VIEW datetime_dim AS\n",
    "SELECT trip_id AS datetime_id,\n",
    "       tpep_pickup_datetime, pick_hour, pick_day, pick_month, pick_year, pick_weekday,\n",
    "       tpep_dropoff_datetime, drop_hour, drop_day, drop_month, drop_year, drop_weekday\n",
    "FROM datetime_dim;\n",
    "\n",
    "-- 2. Join the tables:\n",
    "SELECT f.trip_id,\n",
    "       f.VendorID,\n",
    "       dt.datetime_id,\n",
    "       pcd.passenger_count_id,\n",
    "       tdd.trip_distance_id,\n",
    "       rcd.rate_code_id,\n",
    "       f.store_and_fwd_flag,\n",
    "       pld.pickup_location_id,\n",
    "       dld.dropoff_location_id,\n",
    "       ptd.payment_type_id,\n",
    "       f.fare_amount,\n",
    "       f.extra,\n",
    "       f.mta_tax,\n",
    "       f.tip_amount,\n",
    "       f.tolls_amount,\n",
    "       f.improvement_surcharge,\n",
    "       f.total_amount\n",
    "FROM fact_table f\n",
    "INNER JOIN datetime_dim dt ON f.trip_id = dt.datetime_id\n",
    "INNER JOIN passenger_count_dim pcd ON f.trip_id = pcd.passenger_count_id\n",
    "INNER JOIN trip_distance_dim tdd ON f.trip_id = tdd.trip_distance_id\n",
    "INNER JOIN rate_code_dim rcd ON f.trip_id = rcd.rate_code_id\n",
    "INNER JOIN payment_type_dim ptd ON f.trip_id = ptd.payment_type_id\n",
    "INNER JOIN pickup_location_dim pld ON f.trip_id = pld.pickup_location_id\n",
    "INNER JOIN dropoff_location_dim dld ON f.trip_id = dld.dropoff_location_id;\n",
    "\n",
    "-- Drop temporary views (optional)\n",
    "DROP TEMPORARY VIEW IF EXISTS passenger_count_dim;\n",
    "DROP TEMPORARY VIEW IF EXISTS trip_distance_dim;\n",
    "DROP TEMPORARY VIEW IF EXISTS rate_code_dim;\n",
    "DROP TEMPORARY VIEW IF EXISTS payment_type_dim;\n",
    "DROP TEMPORARY VIEW IF EXISTS pickup_location_dim;\n",
    "DROP TEMPORARY VIEW IF EXISTS dropoff_location_dim;\n",
    "DROP TEMPORARY VIEW IF EXISTS datetime_dim;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "507b0d29-571d-430d-a296-f6ca4b87b6a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>trip_id</th><th>VendorID</th><th>datetime_id</th><th>passenger_count_id</th><th>trip_distance_id</th><th>rate_code_id</th><th>store_and_fwd_flag</th><th>pickup_location_id</th><th>dropoff_location_id</th><th>payment_type_id</th><th>fare_amount</th><th>extra</th><th>mta_tax</th><th>tip_amount</th><th>tolls_amount</th><th>improvement_surcharge</th><th>total_amount</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "trip_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "VendorID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "datetime_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "passenger_count_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "trip_distance_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "rate_code_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "store_and_fwd_flag",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "pickup_location_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "dropoff_location_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "payment_type_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "fare_amount",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "extra",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "mta_tax",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "tip_amount",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "tolls_amount",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "improvement_surcharge",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "total_amount",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- 2. Join the tables:\n",
    "SELECT f.trip_id,\n",
    "       f.VendorID,\n",
    "       dt.datetime_id,\n",
    "       pcd.passenger_count_id,\n",
    "       tdd.trip_distance_id,\n",
    "       rcd.rate_code_id,\n",
    "       f.store_and_fwd_flag,\n",
    "       pld.pickup_location_id,\n",
    "       dld.dropoff_location_id,\n",
    "       ptd.payment_type_id,\n",
    "       f.fare_amount,\n",
    "       f.extra,\n",
    "       f.mta_tax,\n",
    "       f.tip_amount,\n",
    "       f.tolls_amount,\n",
    "       f.improvement_surcharge,\n",
    "       f.total_amount\n",
    "FROM fact_table f\n",
    "INNER JOIN datetime_dim dt ON f.trip_id = dt.datetime_id\n",
    "INNER JOIN passenger_count_dim pcd ON f.trip_id = pcd.passenger_count_id\n",
    "INNER JOIN trip_distance_dim tdd ON f.trip_id = tdd.trip_distance_id\n",
    "INNER JOIN rate_code_dim rcd ON f.trip_id = rcd.rate_code_id\n",
    "INNER JOIN payment_type_dim ptd ON f.trip_id = ptd.payment_type_id\n",
    "INNER JOIN pickup_location_dim pld ON f.trip_id = pld.pickup_location_id\n",
    "INNER JOIN dropoff_location_dim dld ON f.trip_id = dld.dropoff_location_id;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41eafc50-caae-44fc-a6dc-673db44a7b20",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>trip_id</th><th>VendorID</th><th>datetime_id</th><th>passenger_count_id</th><th>trip_distance_id</th><th>rate_code_id</th><th>store_and_fwd_flag</th><th>pickup_location_id</th><th>dropoff_location_id</th><th>payment_type_id</th><th>fare_amount</th><th>extra</th><th>mta_tax</th><th>tip_amount</th><th>tolls_amount</th><th>improvement_surcharge</th><th>total_amount</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "trip_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "VendorID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "datetime_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "passenger_count_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "trip_distance_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "rate_code_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "store_and_fwd_flag",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "pickup_location_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "dropoff_location_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "payment_type_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "fare_amount",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "extra",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "mta_tax",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "tip_amount",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "tolls_amount",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "improvement_surcharge",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "total_amount",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from fact_table\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 546318218290652,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "new-uber-datamodel 2024-07-18 13_08_01",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
